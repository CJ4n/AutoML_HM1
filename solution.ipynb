{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import openml\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# from evaluate_pipeline import evaluate_pipeline\n",
    "# from load_dataset import load_dataset_from_id\n",
    "# from perform_optimazaion_of_pipeline_with_random_search import (\n",
    "#     perform_optimazaion_of_pipeline_with_random_search,\n",
    "# )\n",
    "# from split_dataset import split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utill functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_pipeline_over_params_combinations(\n",
    "    pipeline: Pipeline,\n",
    "    parameters_grid: List[dict],\n",
    "    X: DataFrame,\n",
    "    y: DataFrame,\n",
    "    X_val: DataFrame,\n",
    "    y_val: DataFrame,\n",
    ") -> Pipeline:\n",
    "    best_score = float(\"-inf\")\n",
    "    best_params = None\n",
    "\n",
    "    for params in parameters_grid:\n",
    "        # Update the pipeline parameters\n",
    "        pipeline_params = {f\"model__{key}\": value for key, value in params.items()}\n",
    "        pipeline.set_params(**pipeline_params)\n",
    "\n",
    "        pipeline.fit(X, y)\n",
    "        score = pipeline.score(X_val, y_val)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = pipeline_params\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(\n",
    "    data: pd.DataFrame, class_: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    X: pd.DataFrame = data.drop(labels=class_, axis=1)\n",
    "    y: pd.DataFrame = data[class_]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_id(id: int) -> DataFrame:\n",
    "    return openml.datasets.get_dataset(dataset_id=id).get_data(\n",
    "        dataset_format=\"dataframe\"\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(model: Pipeline, X_test: pd.DataFrame, y_test: pd.Series):\n",
    "    # Ensure X_test and y_test are the correct types\n",
    "    if not isinstance(X_test, (pd.DataFrame, np.ndarray)):\n",
    "        raise ValueError(\"X_test must be a pandas DataFrame or numpy array\")\n",
    "    if not isinstance(y_test, (pd.Series, np.ndarray)):\n",
    "        raise ValueError(\"y_test must be a pandas Series or numpy array\")\n",
    "\n",
    "    # Generating predictions and calculating MSE\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(\n",
    "    pipeline: Pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "):\n",
    "    if not isinstance(X_val, (pd.DataFrame, np.ndarray)):\n",
    "        raise ValueError(\"X_test must be a pandas DataFrame or numpy array\")\n",
    "    if not isinstance(y_val, (pd.Series, np.ndarray)):\n",
    "        raise ValueError(\"y_test must be a pandas Series or numpy array\")\n",
    "\n",
    "    test_score = pipeline.score(X_val, y_val)\n",
    "    train_score = pipeline.score(X, y)\n",
    "    print(\"Parameter set: \" + str(pipeline.named_steps[\"model\"]))\n",
    "    print(\"Test score R^2: \" + str(test_score))\n",
    "    print(\"Train score R^2: \" + str(train_score))\n",
    "    calculate_mse(pipeline, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AutoML_HM1\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AutoML_HM1\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AutoML_HM1\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AutoML_HM1\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fish_market_dataset_id = (\n",
    "    43308  # https://www.openml.org/search?type=data&id=43308&sort=runs&status=active\n",
    ")\n",
    "liver_disorders_dataset_id = (\n",
    "    8  # https://www.openml.org/search?type=data&id=8&sort=runs&status=active\n",
    ")\n",
    "diabetes_dataset_id = (\n",
    "    44223  # https://www.openml.org/search?type=data&id=44223&sort=runs&status=active\n",
    ")\n",
    "\n",
    "lisbona_house_prices_dataset_id = (\n",
    "    43660  # https://www.openml.org/search?type=data&id=43660&sort=runs&status=active\n",
    ")\n",
    "\n",
    "\n",
    "fish_market_dataset: DataFrame = load_dataset_from_id(id=fish_market_dataset_id)\n",
    "fish_market_regression_class = \"Weight\"\n",
    "\n",
    "liver_disorders_dataset: DataFrame = load_dataset_from_id(id=liver_disorders_dataset_id)\n",
    "liver_disorders_regression_class = \"drinks\"\n",
    "diabetes_dataset: DataFrame = load_dataset_from_id(id=diabetes_dataset_id)\n",
    "diabetes_regression_class = \"class\"\n",
    "\n",
    "lisbona_house_prices_dataset: DataFrame = load_dataset_from_id(\n",
    "    id=lisbona_house_prices_dataset_id\n",
    ")\n",
    "lisbona_house_prices_regression_class = \"Price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train_fish_market,\n",
    "    X_test_fish_market,\n",
    "    y_train_fish_market,\n",
    "    y_test_fish_market,\n",
    ") = split_dataset(data=fish_market_dataset, class_=fish_market_regression_class)\n",
    "\n",
    "(\n",
    "    X_train_liver_disorders,\n",
    "    X_test_liver_disorders,\n",
    "    y_train_liver_disorders,\n",
    "    y_test_liver_disorders,\n",
    ") = split_dataset(data=liver_disorders_dataset, class_=liver_disorders_regression_class)\n",
    "\n",
    "X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = split_dataset(\n",
    "    diabetes_dataset, diabetes_regression_class\n",
    ")\n",
    "\n",
    "(\n",
    "    X_train_lisbona_house_prices,\n",
    "    X_test_lisbona_house_prices,\n",
    "    y_train_lisbona_house_prices,\n",
    "    y_test_lisbona_house_prices,\n",
    ") = split_dataset(lisbona_house_prices_dataset, lisbona_house_prices_regression_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create generic column transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), (\"scale\", MinMaxScaler())]\n",
    ")\n",
    "cat_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"one-hot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "        (\"cat_pipeline\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create models TODO: create also otehr pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet()\n",
    "knn = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree_pipeline = Pipeline(\n",
    "    steps=[(\"column_transformer\", col_trans), (\"model\", decision_tree)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters space\n",
    "ccp_alpha_values = [i * 0.1 for i in range(11)]\n",
    "max_depth_values = range(1, 31, 1)\n",
    "min_samples_split_values = range(2, 61, 1)\n",
    "min_samples_leaf_values = range(1, 61, 1)\n",
    "\n",
    "all_combinations = list(\n",
    "    itertools.product(\n",
    "        ccp_alpha_values,\n",
    "        max_depth_values,\n",
    "        min_samples_split_values,\n",
    "        min_samples_leaf_values,\n",
    "    )\n",
    ")\n",
    "selected_combinations: List[Tuple[float, int, int, int]] = random.sample(\n",
    "    all_combinations, 100\n",
    ")\n",
    "parameter_names = [\"ccp_alpha\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\"]\n",
    "\n",
    "parameters_grid = [\n",
    "    dict(zip(parameter_names, combination)) for combination in selected_combinations\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate the model on fish market dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=29, min_samples_leaf=4)\n",
      "Test score R^2: 0.8287637344030986\n",
      "Train score R^2: 0.7815269691092457\n",
      "Mean Squared Error: 24356.524298952434\n"
     ]
    }
   ],
   "source": [
    "ecision_tree_pipeline = optimize_pipeline_over_params_combinations(\n",
    "    pipeline=decision_tree_pipeline,\n",
    "    parameters_grid=parameters_grid,\n",
    "    X=X_train_fish_market,\n",
    "    y=y_train_fish_market,\n",
    "    X_val=X_test_fish_market,\n",
    "    y_val=y_test_fish_market,\n",
    ")\n",
    "\n",
    "evaluate_pipeline(\n",
    "    pipeline=decision_tree_pipeline,\n",
    "    X=X_train_fish_market,\n",
    "    y=y_train_fish_market,\n",
    "    X_val=X_test_fish_market,\n",
    "    y_val=y_test_fish_market,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model on liver disorders dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.9, max_depth=22, min_samples_split=20)\n",
      "Test score R^2: 0.5555960561491127\n",
      "Train score R^2: 0.5362524536474875\n",
      "Mean Squared Error: 25006470422.091454\n"
     ]
    }
   ],
   "source": [
    "decision_tree_pipeline = optimize_pipeline_over_params_combinations(\n",
    "    pipeline=decision_tree_pipeline,\n",
    "    parameters_grid=parameters_grid,  \n",
    "    X=X_train_lisbona_house_prices,\n",
    "    y=y_train_lisbona_house_prices,\n",
    "    X_val=X_test_lisbona_house_prices,\n",
    "    y_val=y_test_lisbona_house_prices,\n",
    ")\n",
    "\n",
    "evaluate_pipeline(\n",
    "    pipeline=decision_tree_pipeline,\n",
    "    X=X_train_lisbona_house_prices,\n",
    "    y=y_train_lisbona_house_prices,\n",
    "    X_val=X_test_lisbona_house_prices,\n",
    "    y_val=y_test_lisbona_house_prices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
