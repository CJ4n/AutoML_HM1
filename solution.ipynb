{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- ile potrzeba iteracji (random seach, bayes) do uzyskania stabilncyh wyników\n",
    "- trzeba się zastanowić jakie zakres hyper parametrów bierzemy\n",
    "- określić i przeanalizować tunowalność CAŁYCH algorytmów\n",
    "- sprawdić jak zmiana seed w random search wpływa na wyniki tunowalnośći (sampling bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "\n",
    "from utills.dataset import load_dataset_from_id, split_dataset\n",
    "from utills.pipeline import evaluate_pipeline_on_datasets, get_column_transformer, get_bayes_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utill functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bayes_config(\n",
    "    pipeline: Pipeline,\n",
    "    search_space: Dict[str, Any],\n",
    "    X: DataFrame,\n",
    "    y: DataFrame,\n",
    "    n_iter=100,\n",
    "):\n",
    "    opt: get_bayes_model(\n",
    "        pipeline,\n",
    "        search_space\n",
    "    )\n",
    "    opt.fit(X, y)\n",
    "    return dict(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_pipeline_over_params_combinations(\n",
    "#     pipeline: Pipeline,\n",
    "#     parameters_grid: List[dict],\n",
    "#     X: DataFrame,\n",
    "#     y: DataFrame,\n",
    "#     X_val: DataFrame,\n",
    "#     y_val: DataFrame,\n",
    "# ) -> Pipeline:\n",
    "#     # thats the teta^(j)*\n",
    "#     best_score = float(\"-inf\")\n",
    "#     best_params = None\n",
    "\n",
    "#     for params in parameters_grid:\n",
    "#         # Update the pipeline parameters\n",
    "#         pipeline_params = {f\"{key}\": value for key, value in params.items()}\n",
    "#         pipeline.set_params(**pipeline_params)\n",
    "\n",
    "#         pipeline.fit(X, y)\n",
    "#         score = pipeline.score(X_val, y_val)\n",
    "\n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_params = pipeline_params\n",
    "\n",
    "#     pipeline.set_params(**best_params)\n",
    "#     return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(\n",
    "    model: Pipeline, X_train, y_train, X_test, y_test\n",
    ") -> float:\n",
    "    model.fit(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "    )\n",
    "    return model.score(\n",
    "        X=X_test,\n",
    "        y=y_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    train_datasets: List[Tuple[DataFrame, Series]],\n",
    "    test_datasets: List[Tuple[DataFrame, Series]],\n",
    "    model: Pipeline,\n",
    "    config,\n",
    ") -> List[float]:\n",
    "    performances: List[float] = []\n",
    "    for (X_train, y_train), (X_test, y_test) in zip(train_datasets, test_datasets):\n",
    "        model.set_params(**config)\n",
    "        performance: float = evaluate_model_performance(\n",
    "            model=model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        performances.append(performance)\n",
    "    return performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_configuration_for_all_datasets(\n",
    "    config_space,\n",
    "    train_datasets: List[Tuple[DataFrame, Series]],\n",
    "    test_datasets: List[Tuple[DataFrame, Series]],\n",
    "    model: Pipeline,\n",
    "    summary_func,\n",
    "):\n",
    "    best_config = None\n",
    "    best_summary_score = float(\"0\")\n",
    "    last_idx_of_config_with_significant_imporvement = -1\n",
    "    for idx, config in enumerate(config_space):\n",
    "        performances = experiment(\n",
    "            train_datasets=train_datasets,\n",
    "            test_datasets=test_datasets,\n",
    "            model=model,\n",
    "            config=config,\n",
    "        )\n",
    "        summary_score = summary_func(performances)\n",
    "\n",
    "        if summary_score > best_summary_score:\n",
    "            if abs(summary_score - best_summary_score) > 0.01:\n",
    "                last_idx_of_config_with_significant_imporvement = idx\n",
    "                best_summary_score = summary_score\n",
    "                best_config = config\n",
    "\n",
    "    return (best_config, last_idx_of_config_with_significant_imporvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_config_for_dataset(\n",
    "    config_space,\n",
    "    train_dataset: Tuple[DataFrame, Series],\n",
    "    test_dataset: Tuple[DataFrame, Series],\n",
    "    model: Pipeline,\n",
    "):\n",
    "    best_config = None\n",
    "    best_score = float(\"0\")\n",
    "\n",
    "    for config in config_space:\n",
    "        # model = get_model_func()\n",
    "        model.set_params(**config)\n",
    "        score: float = evaluate_model_performance(\n",
    "            model=model,\n",
    "            X_train=train_dataset[0],\n",
    "            y_train=train_dataset[1],\n",
    "            X_test=test_dataset[0],\n",
    "            y_test=test_dataset[1],\n",
    "        )\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_config = config\n",
    "\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fish_market_dataset_id = (\n",
    "    43308  # https://www.openml.org/search?type=data&id=43308&sort=runs&status=active\n",
    ")\n",
    "liver_disorders_dataset_id = (\n",
    "    8  # https://www.openml.org/search?type=data&id=8&sort=runs&status=active\n",
    ")\n",
    "diabetes_dataset_id = (\n",
    "    44223  # https://www.openml.org/search?type=data&id=44223&sort=runs&status=active\n",
    ")\n",
    "\n",
    "lisbona_house_prices_dataset_id = (\n",
    "    43660  # https://www.openml.org/search?type=data&id=43660&sort=runs&status=active\n",
    ")\n",
    "\n",
    "\n",
    "fish_market_dataset: DataFrame = load_dataset_from_id(id=fish_market_dataset_id)\n",
    "fish_market_regression_class = \"Weight\"\n",
    "\n",
    "liver_disorders_dataset: DataFrame = load_dataset_from_id(id=liver_disorders_dataset_id)\n",
    "liver_disorders_regression_class = \"drinks\"\n",
    "diabetes_dataset: DataFrame = load_dataset_from_id(id=diabetes_dataset_id)\n",
    "diabetes_regression_class = \"class\"\n",
    "\n",
    "lisbona_house_prices_dataset: DataFrame = load_dataset_from_id(\n",
    "    id=lisbona_house_prices_dataset_id\n",
    ")\n",
    "lisbona_house_prices_regression_class = \"Price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train_fish_market,\n",
    "    X_test_fish_market,\n",
    "    y_train_fish_market,\n",
    "    y_test_fish_market,\n",
    ") = split_dataset(data=fish_market_dataset, class_=fish_market_regression_class)\n",
    "\n",
    "(\n",
    "    X_train_liver_disorders,\n",
    "    X_test_liver_disorders,\n",
    "    y_train_liver_disorders,\n",
    "    y_test_liver_disorders,\n",
    ") = split_dataset(data=liver_disorders_dataset, class_=liver_disorders_regression_class)\n",
    "\n",
    "X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = split_dataset(\n",
    "    diabetes_dataset, diabetes_regression_class\n",
    ")\n",
    "\n",
    "(\n",
    "    X_train_lisbona_house_prices,\n",
    "    X_test_lisbona_house_prices,\n",
    "    y_train_lisbona_house_prices,\n",
    "    y_test_lisbona_house_prices,\n",
    ") = split_dataset(lisbona_house_prices_dataset, lisbona_house_prices_regression_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_datasets() -> List[Tuple[DataFrame, Series]]:\n",
    "    return [\n",
    "        (X_train_fish_market, y_train_fish_market),\n",
    "        (X_train_liver_disorders, y_train_liver_disorders),\n",
    "        (X_train_diabetes, y_train_diabetes),\n",
    "        (X_train_lisbona_house_prices, y_train_lisbona_house_prices),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_datasets():\n",
    "    return [\n",
    "        (X_test_fish_market, y_test_fish_market),\n",
    "        (X_test_liver_disorders, y_test_liver_disorders),\n",
    "        (X_test_diabetes, y_test_diabetes),\n",
    "        (X_test_lisbona_house_prices, y_test_lisbona_house_prices),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create generic column transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision_tree_pipeline() -> Pipeline:\n",
    "    decision_tree = DecisionTreeRegressor()\n",
    "    col_trans: ColumnTransformer = get_column_transformer()\n",
    "    decision_tree_pipeline = Pipeline(\n",
    "        steps=[(\"column_transformer\", col_trans), (\"model\", decision_tree)]\n",
    "    )\n",
    "    return decision_tree_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration_grid_decision_tree():\n",
    "    # parameters space\n",
    "    random.seed(42)\n",
    "    ccp_alpha_values = [i * 0.1 for i in range(11)]\n",
    "\n",
    "    max_depth_values = range(1, 31, 1)\n",
    "\n",
    "    min_samples_split_values = range(2, 61, 1)\n",
    "\n",
    "    min_samples_leaf_values = range(1, 61, 1)\n",
    "\n",
    "    all_combinations = list(\n",
    "        itertools.product(\n",
    "            ccp_alpha_values,\n",
    "            max_depth_values,\n",
    "            min_samples_split_values,\n",
    "            min_samples_leaf_values,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    selected_combinations: List[Tuple[float, int, int, int]] = random.sample(\n",
    "        all_combinations, 1000\n",
    "    )\n",
    "\n",
    "    parameter_names = [\n",
    "        \"model__ccp_alpha\",\n",
    "        \"model__max_depth\",\n",
    "        \"model__min_samples_split\",\n",
    "        \"model__min_samples_leaf\",\n",
    "    ]\n",
    "\n",
    "    config_grid = [\n",
    "        dict(zip(parameter_names, combination)) for combination in selected_combinations\n",
    "    ]\n",
    "\n",
    "    return config_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets: List[Tuple[DataFrame, Series]] = get_train_datasets()\n",
    "test_datasets: List[Tuple[DataFrame, Series]] = get_test_datasets()\n",
    "decison_tree_pipeline: Pipeline = get_decision_tree_pipeline()\n",
    "configuration_grid_decision_tree = get_configuration_grid_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find teta*\n",
    "\n",
    "(\n",
    "    optimal_config_decision_tree,\n",
    "    last_idx_of_config_with_significant_imporvement,\n",
    ") = find_optimal_configuration_for_all_datasets(\n",
    "    config_space=configuration_grid_decision_tree,\n",
    "    train_datasets=train_datasets,\n",
    "    test_datasets=test_datasets,\n",
    "    model=decison_tree_pipeline,\n",
    "    summary_func=np.mean,  # Or np.median for a more robust approach\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__ccp_alpha': 0.7000000000000001, 'model__max_depth': 15, 'model__min_samples_split': 32, 'model__min_samples_leaf': 5}\n",
      "240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;column_transformer&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;num_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scale&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E4A46250&gt;),\n",
       "                                                 (&#x27;cat_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;one-hot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E71DFF90&gt;)])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 DecisionTreeRegressor(ccp_alpha=0.7000000000000001,\n",
       "                                       max_depth=15, min_samples_leaf=5,\n",
       "                                       min_samples_split=32))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;column_transformer&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;num_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scale&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E4A46250&gt;),\n",
       "                                                 (&#x27;cat_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;one-hot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E71DFF90&gt;)])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 DecisionTreeRegressor(ccp_alpha=0.7000000000000001,\n",
       "                                       max_depth=15, min_samples_leaf=5,\n",
       "                                       min_samples_split=32))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;num_pipeline&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute&#x27;, SimpleImputer()),\n",
       "                                                 (&#x27;scale&#x27;, MinMaxScaler())]),\n",
       "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E4A46250&gt;),\n",
       "                                (&#x27;cat_pipeline&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;one-hot&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse=False))]),\n",
       "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E71DFF90&gt;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num_pipeline</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E4A46250&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat_pipeline</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E71DFF90&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse=False)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(ccp_alpha=0.7000000000000001, max_depth=15,\n",
       "                      min_samples_leaf=5, min_samples_split=32)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('column_transformer',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('num_pipeline',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scale',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E4A46250>),\n",
       "                                                 ('cat_pipeline',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('one-hot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000001F6E71DFF90>)])),\n",
       "                ('model',\n",
       "                 DecisionTreeRegressor(ccp_alpha=0.7000000000000001,\n",
       "                                       max_depth=15, min_samples_leaf=5,\n",
       "                                       min_samples_split=32))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(optimal_config_decision_tree)\n",
    "print(last_idx_of_config_with_significant_imporvement)\n",
    "optimal_decision_tree = get_decision_tree_pipeline()\n",
    "optimal_decision_tree.set_params(**optimal_config_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_configs_for_each_set(\n",
    "    pipeline: Pipeline, config_space, train_datasets, test_datasets\n",
    "):\n",
    "    best_configs = []\n",
    "    for train_dataset, test_dataset in zip(train_datasets, test_datasets):\n",
    "        best_config = find_optimal_config_for_dataset(\n",
    "            config_space=config_space,\n",
    "            train_dataset=train_dataset,\n",
    "            test_dataset=test_dataset,\n",
    "            model=pipeline,\n",
    "        )\n",
    "        best_configs.append(best_config)\n",
    "        pipeline.set_params(**best_config)\n",
    "        pipeline.fit(train_dataset[0], train_dataset[1])\n",
    "        print(\"score: \" + str(pipeline.score(test_dataset[0], test_dataset[1])))\n",
    "        print(\"best config: \" + str(best_config))\n",
    "    return best_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9691417799047347\n",
      "best config: {'model__ccp_alpha': 0.5, 'model__max_depth': 11, 'model__min_samples_split': 2, 'model__min_samples_leaf': 4}\n",
      "score: 0.1878010532545623\n",
      "best config: {'model__ccp_alpha': 0.2, 'model__max_depth': 25, 'model__min_samples_split': 39, 'model__min_samples_leaf': 44}\n",
      "score: 0.4943273349113829\n",
      "best config: {'model__ccp_alpha': 0.5, 'model__max_depth': 28, 'model__min_samples_split': 26, 'model__min_samples_leaf': 6}\n",
      "score: 0.7137185531740955\n",
      "best config: {'model__ccp_alpha': 0.8, 'model__max_depth': 23, 'model__min_samples_split': 13, 'model__min_samples_leaf': 4}\n"
     ]
    }
   ],
   "source": [
    "# find tate^(j)*\n",
    "best_configs_for_each_dataset = get_best_configs_for_each_set(\n",
    "    pipeline=get_decision_tree_pipeline(),\n",
    "    config_space=get_configuration_grid_decision_tree(),\n",
    "    train_datasets=get_train_datasets(),\n",
    "    test_datasets=get_test_datasets(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tunability_on_each_set(train_datasets, test_datasets, best_configs):\n",
    "    tunability = []\n",
    "    for train_dataset, test_dataset, best_config in zip(\n",
    "        train_datasets, test_datasets, best_configs\n",
    "    ):\n",
    "        optimal_decision_tree.fit(train_dataset[0], train_dataset[1])\n",
    "        best_decision_tree_for_dataset = get_decision_tree_pipeline()\n",
    "        best_decision_tree_for_dataset.set_params(**best_config)\n",
    "        best_decision_tree_for_dataset.fit(train_dataset[0], train_dataset[1])\n",
    "        tunability_on_dataset = best_decision_tree_for_dataset.score(\n",
    "            test_dataset[0], test_dataset[1]\n",
    "        ) - optimal_decision_tree.score(test_dataset[0], test_dataset[1])\n",
    "        tunability.append(tunability_on_dataset)\n",
    "        print(\"d^j: \" + str(tunability_on_dataset))\n",
    "    return tunability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d^j: 0.02520841681490149\n",
      "d^j: 0.12012499127253262\n",
      "d^j: 0.035673021375325464\n",
      "d^j: 0.04380643997784195\n"
     ]
    }
   ],
   "source": [
    "# find d^j\n",
    "tunability_on_datasets = calculate_tunability_on_each_set(\n",
    "    get_train_datasets(), get_test_datasets(), best_configs_for_each_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tunability data\n",
    "tunability_data = [tunability_on_datasets]\n",
    "\n",
    "# create box plot\n",
    "sns.boxplot(data=tunability_data)\n",
    "\n",
    "# add strip plot\n",
    "sns.stripplot(data=tunability_data, color=\"black\", size=4)\n",
    "\n",
    "# add labels and title\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"Tunability\")\n",
    "plt.title(\"Tunability per Dataset\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated tunability d: 0.05620321736015038\n"
     ]
    }
   ],
   "source": [
    "# aggregated tunability d\n",
    "print(\"Aggregated tunability d: \" + str(np.mean(tunability_on_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_for_pipeline(pipeline: Pipeline, train_datasets, test_datasets):\n",
    "    scores = []\n",
    "    for train_dataset, test_dataset in zip(train_datasets, test_datasets):\n",
    "        pipeline.fit(train_dataset[0], train_dataset[1])\n",
    "        score = pipeline.score(test_dataset[0], test_dataset[1])\n",
    "        scores.append(score)\n",
    "        print(\"score: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9447855527094973\n",
      "score: 0.06767606198202969\n",
      "score: 0.45865431353605746\n",
      "score: 0.6699121131962535\n"
     ]
    }
   ],
   "source": [
    "calculate_scores_for_pipeline(\n",
    "    optimal_decision_tree, get_train_datasets(), get_test_datasets()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=6, min_samples_leaf=19,\n",
      "                      min_samples_split=58)\n",
      "Test score R^2: 0.7815269691092457\n",
      "Train score R^2: 0.7815269691092457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 26856.500884781224\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=6, min_samples_leaf=19,\n",
      "                      min_samples_split=58)\n",
      "Test score R^2: 0.15528101406816253\n",
      "Train score R^2: 0.15528101406816253\n",
      "Mean Squared Error: 9.48162852008955\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=6, min_samples_leaf=19,\n",
      "                      min_samples_split=58)\n",
      "Test score R^2: 0.5221465690813645\n",
      "Train score R^2: 0.5221465690813645\n",
      "Mean Squared Error: 2903.6276381318785\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=6, min_samples_leaf=19,\n",
      "                      min_samples_split=58)\n",
      "Test score R^2: 0.5595820029015193\n",
      "Train score R^2: 0.5595820029015193\n",
      "Mean Squared Error: 89991302260.828\n",
      "Parameter set: DecisionTreeRegressor(max_depth=15, min_samples_leaf=12, min_samples_split=50)\n",
      "Test score R^2: 0.789919150538415\n",
      "Train score R^2: 0.789919150538415\n",
      "Mean Squared Error: 25824.864956727335\n",
      "Parameter set: DecisionTreeRegressor(max_depth=15, min_samples_leaf=12, min_samples_split=50)\n",
      "Test score R^2: 0.24069100272757193\n",
      "Train score R^2: 0.24069100272757193\n",
      "Mean Squared Error: 8.522935986997926\n",
      "Parameter set: DecisionTreeRegressor(max_depth=15, min_samples_leaf=12, min_samples_split=50)\n",
      "Test score R^2: 0.5450218184758296\n",
      "Train score R^2: 0.5450218184758296\n",
      "Mean Squared Error: 2764.6285181648223\n",
      "Parameter set: DecisionTreeRegressor(max_depth=15, min_samples_leaf=12, min_samples_split=50)\n",
      "Test score R^2: 0.7132709369960079\n",
      "Train score R^2: 0.7132709369960079\n",
      "Mean Squared Error: 58587800557.08415\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.5, max_depth=13, min_samples_leaf=59,\n",
      "                      min_samples_split=56)\n",
      "Test score R^2: 0.6191250374142274\n",
      "Train score R^2: 0.6191250374142274\n",
      "Mean Squared Error: 46820.28133161539\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.5, max_depth=13, min_samples_leaf=59,\n",
      "                      min_samples_split=56)\n",
      "Test score R^2: 0.11406948450062537\n",
      "Train score R^2: 0.11406948450062537\n",
      "Mean Squared Error: 9.944211249508166\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.5, max_depth=13, min_samples_leaf=59,\n",
      "                      min_samples_split=56)\n",
      "Test score R^2: 0.43254023226744\n",
      "Train score R^2: 0.43254023226744\n",
      "Mean Squared Error: 3448.111405098839\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.5, max_depth=13, min_samples_leaf=59,\n",
      "                      min_samples_split=56)\n",
      "Test score R^2: 0.4859759704370552\n",
      "Train score R^2: 0.4859759704370552\n",
      "Mean Squared Error: 105031338679.34148\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=26, min_samples_leaf=36,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.7765160292190798\n",
      "Train score R^2: 0.7765160292190798\n",
      "Mean Squared Error: 27472.4868078269\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=26, min_samples_leaf=36,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.11734665711716263\n",
      "Train score R^2: 0.11734665711716263\n",
      "Mean Squared Error: 9.907426314087378\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=26, min_samples_leaf=36,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.4794698155499745\n",
      "Train score R^2: 0.4794698155499745\n",
      "Mean Squared Error: 3162.94857849065\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=26, min_samples_leaf=36,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.5296159165459413\n",
      "Train score R^2: 0.5296159165459413\n",
      "Mean Squared Error: 96114319831.78322\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=13, min_samples_leaf=47,\n",
      "                      min_samples_split=15)\n",
      "Test score R^2: 0.674980824248782\n",
      "Train score R^2: 0.674980824248782\n",
      "Mean Squared Error: 39954.028859050595\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=13, min_samples_leaf=47,\n",
      "                      min_samples_split=15)\n",
      "Test score R^2: 0.11406948450062537\n",
      "Train score R^2: 0.11406948450062537\n",
      "Mean Squared Error: 9.944211249508166\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=13, min_samples_leaf=47,\n",
      "                      min_samples_split=15)\n",
      "Test score R^2: 0.4634973366499989\n",
      "Train score R^2: 0.4634973366499989\n",
      "Mean Squared Error: 3260.0037175409007\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=13, min_samples_leaf=47,\n",
      "                      min_samples_split=15)\n",
      "Test score R^2: 0.5157035955123616\n",
      "Train score R^2: 0.5157035955123616\n",
      "Mean Squared Error: 98957046276.9575\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=23, min_samples_leaf=13,\n",
      "                      min_samples_split=41)\n",
      "Test score R^2: 0.789919150538415\n",
      "Train score R^2: 0.789919150538415\n",
      "Mean Squared Error: 25824.864956727335\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=23, min_samples_leaf=13,\n",
      "                      min_samples_split=41)\n",
      "Test score R^2: 0.2639431943639722\n",
      "Train score R^2: 0.2639431943639722\n",
      "Mean Squared Error: 8.261939552626234\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=23, min_samples_leaf=13,\n",
      "                      min_samples_split=41)\n",
      "Test score R^2: 0.5565776581396269\n",
      "Train score R^2: 0.5565776581396269\n",
      "Mean Squared Error: 2694.410636993356\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=23, min_samples_leaf=13,\n",
      "                      min_samples_split=41)\n",
      "Test score R^2: 0.7132709369960079\n",
      "Train score R^2: 0.7132709369960079\n",
      "Mean Squared Error: 58587800557.08415\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=1, min_samples_leaf=28,\n",
      "                      min_samples_split=44)\n",
      "Test score R^2: 0.6950440400288784\n",
      "Train score R^2: 0.6950440400288784\n",
      "Mean Squared Error: 37487.69344843805\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=1, min_samples_leaf=28,\n",
      "                      min_samples_split=44)\n",
      "Test score R^2: 0.11734665711716263\n",
      "Train score R^2: 0.11734665711716263\n",
      "Mean Squared Error: 9.907426314087378\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=1, min_samples_leaf=28,\n",
      "                      min_samples_split=44)\n",
      "Test score R^2: 0.30430942765565605\n",
      "Train score R^2: 0.30430942765565605\n",
      "Mean Squared Error: 4227.292811445302\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.2, max_depth=1, min_samples_leaf=28,\n",
      "                      min_samples_split=44)\n",
      "Test score R^2: 0.45902318512499396\n",
      "Train score R^2: 0.45902318512499396\n",
      "Mean Squared Error: 110538643707.22058\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=24, min_samples_leaf=57,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.6328118884390866\n",
      "Train score R^2: 0.6328118884390866\n",
      "Mean Squared Error: 45137.78109275156\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=24, min_samples_leaf=57,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.11406948450062537\n",
      "Train score R^2: 0.11406948450062537\n",
      "Mean Squared Error: 9.944211249508166\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=24, min_samples_leaf=57,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.43684482413295955\n",
      "Train score R^2: 0.43684482413295955\n",
      "Mean Squared Error: 3421.9549916404862\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=24, min_samples_leaf=57,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.48975175804838733\n",
      "Train score R^2: 0.48975175804838733\n",
      "Mean Squared Error: 104259826056.23653\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=22, min_samples_leaf=44,\n",
      "                      min_samples_split=31)\n",
      "Test score R^2: 0.674980824248782\n",
      "Train score R^2: 0.674980824248782\n",
      "Mean Squared Error: 39954.028859050595\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=22, min_samples_leaf=44,\n",
      "                      min_samples_split=31)\n",
      "Test score R^2: 0.1809270356624959\n",
      "Train score R^2: 0.1809270356624959\n",
      "Mean Squared Error: 9.193762313900969\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=22, min_samples_leaf=44,\n",
      "                      min_samples_split=31)\n",
      "Test score R^2: 0.47574569176031156\n",
      "Train score R^2: 0.47574569176031156\n",
      "Mean Squared Error: 3185.5778368862666\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=22, min_samples_leaf=44,\n",
      "                      min_samples_split=31)\n",
      "Test score R^2: 0.5253118659699338\n",
      "Train score R^2: 0.5253118659699338\n",
      "Mean Squared Error: 96993773257.58125\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.8, max_depth=10, min_samples_leaf=15,\n",
      "                      min_samples_split=58)\n",
      "Test score R^2: 0.7815269691092457\n",
      "Train score R^2: 0.7815269691092457\n",
      "Mean Squared Error: 26856.500884781224\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.8, max_depth=10, min_samples_leaf=15,\n",
      "                      min_samples_split=58)\n",
      "Test score R^2: 0.11734665711716263\n",
      "Train score R^2: 0.11734665711716263\n",
      "Mean Squared Error: 9.907426314087378\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.8, max_depth=10, min_samples_leaf=15,\n",
      "                      min_samples_split=58)\n",
      "Test score R^2: 0.5227000242383976\n",
      "Train score R^2: 0.5227000242383976\n",
      "Mean Squared Error: 2900.264624315406\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.8, max_depth=10, min_samples_leaf=15,\n",
      "                      min_samples_split=58)\n",
      "Test score R^2: 0.5595820029015193\n",
      "Train score R^2: 0.5595820029015193\n",
      "Mean Squared Error: 89991302260.828\n",
      "Parameter set: DecisionTreeRegressor(max_depth=19, min_samples_leaf=54, min_samples_split=50)\n",
      "Test score R^2: 0.6441798202604005\n",
      "Train score R^2: 0.6441798202604005\n",
      "Mean Squared Error: 43740.34146474588\n",
      "Parameter set: DecisionTreeRegressor(max_depth=19, min_samples_leaf=54, min_samples_split=50)\n",
      "Test score R^2: 0.15650060819389422\n",
      "Train score R^2: 0.15650060819389422\n",
      "Mean Squared Error: 9.46793906994334\n",
      "Parameter set: DecisionTreeRegressor(max_depth=19, min_samples_leaf=54, min_samples_split=50)\n",
      "Test score R^2: 0.43684482413295955\n",
      "Train score R^2: 0.43684482413295955\n",
      "Mean Squared Error: 3421.9549916404862\n",
      "Parameter set: DecisionTreeRegressor(max_depth=19, min_samples_leaf=54, min_samples_split=50)\n",
      "Test score R^2: 0.4931640505110051\n",
      "Train score R^2: 0.4931640505110051\n",
      "Mean Squared Error: 103562586968.75864\n",
      "Parameter set: DecisionTreeRegressor(max_depth=18, min_samples_leaf=30, min_samples_split=40)\n",
      "Test score R^2: 0.7815269691092457\n",
      "Train score R^2: 0.7815269691092457\n",
      "Mean Squared Error: 26856.500884781224\n",
      "Parameter set: DecisionTreeRegressor(max_depth=18, min_samples_leaf=30, min_samples_split=40)\n",
      "Test score R^2: 0.1981026136312567\n",
      "Train score R^2: 0.1981026136312567\n",
      "Mean Squared Error: 9.000973407022101\n",
      "Parameter set: DecisionTreeRegressor(max_depth=18, min_samples_leaf=30, min_samples_split=40)\n",
      "Test score R^2: 0.48146213781166225\n",
      "Train score R^2: 0.48146213781166225\n",
      "Mean Squared Error: 3150.8424354585054\n",
      "Parameter set: DecisionTreeRegressor(max_depth=18, min_samples_leaf=30, min_samples_split=40)\n",
      "Test score R^2: 0.5362524536474875\n",
      "Train score R^2: 0.5362524536474875\n",
      "Mean Squared Error: 94758265764.49924\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=26, min_samples_leaf=54,\n",
      "                      min_samples_split=31)\n",
      "Test score R^2: 0.6441798202604005\n",
      "Train score R^2: 0.6441798202604005\n",
      "Mean Squared Error: 43740.34146474588\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=26, min_samples_leaf=54,\n",
      "                      min_samples_split=31)\n",
      "Test score R^2: 0.15650060819389422\n",
      "Train score R^2: 0.15650060819389422\n",
      "Mean Squared Error: 9.46793906994334\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=26, min_samples_leaf=54,\n",
      "                      min_samples_split=31)\n",
      "Test score R^2: 0.43684482413295955\n",
      "Train score R^2: 0.43684482413295955\n",
      "Mean Squared Error: 3421.9549916404862\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=26, min_samples_leaf=54,\n",
      "                      min_samples_split=31)\n",
      "Test score R^2: 0.4931640505110051\n",
      "Train score R^2: 0.4931640505110051\n",
      "Mean Squared Error: 103562586968.75864\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=10, min_samples_leaf=58,\n",
      "                      min_samples_split=32)\n",
      "Test score R^2: 0.6247132560729404\n",
      "Train score R^2: 0.6247132560729404\n",
      "Mean Squared Error: 46133.33155689871\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=10, min_samples_leaf=58,\n",
      "                      min_samples_split=32)\n",
      "Test score R^2: 0.11406948450062537\n",
      "Train score R^2: 0.11406948450062537\n",
      "Mean Squared Error: 9.944211249508166\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=10, min_samples_leaf=58,\n",
      "                      min_samples_split=32)\n",
      "Test score R^2: 0.43254023226744\n",
      "Train score R^2: 0.43254023226744\n",
      "Mean Squared Error: 3448.111405098839\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=10, min_samples_leaf=58,\n",
      "                      min_samples_split=32)\n",
      "Test score R^2: 0.48975175804838733\n",
      "Train score R^2: 0.48975175804838733\n",
      "Mean Squared Error: 104259826056.23653\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=18, min_samples_leaf=6,\n",
      "                      min_samples_split=51)\n",
      "Test score R^2: 0.789919150538415\n",
      "Train score R^2: 0.789919150538415\n",
      "Mean Squared Error: 25824.864956727335\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=18, min_samples_leaf=6,\n",
      "                      min_samples_split=51)\n",
      "Test score R^2: 0.19718855402571\n",
      "Train score R^2: 0.19718855402571\n",
      "Mean Squared Error: 9.011233355915582\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=18, min_samples_leaf=6,\n",
      "                      min_samples_split=51)\n",
      "Test score R^2: 0.5747918915000031\n",
      "Train score R^2: 0.5747918915000031\n",
      "Mean Squared Error: 2583.733705594328\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.4, max_depth=18, min_samples_leaf=6,\n",
      "                      min_samples_split=51)\n",
      "Test score R^2: 0.7423932765727297\n",
      "Train score R^2: 0.7423932765727297\n",
      "Mean Squared Error: 52637187092.92724\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.9, max_depth=30, min_samples_leaf=27,\n",
      "                      min_samples_split=24)\n",
      "Test score R^2: 0.789919150538415\n",
      "Train score R^2: 0.789919150538415\n",
      "Mean Squared Error: 25824.864956727335\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.9, max_depth=30, min_samples_leaf=27,\n",
      "                      min_samples_split=24)\n",
      "Test score R^2: 0.11734665711716263\n",
      "Train score R^2: 0.11734665711716263\n",
      "Mean Squared Error: 9.907426314087378\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.9, max_depth=30, min_samples_leaf=27,\n",
      "                      min_samples_split=24)\n",
      "Test score R^2: 0.5213709259235062\n",
      "Train score R^2: 0.5213709259235062\n",
      "Mean Squared Error: 2908.3407546750736\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.9, max_depth=30, min_samples_leaf=27,\n",
      "                      min_samples_split=24)\n",
      "Test score R^2: 0.5504928637189375\n",
      "Train score R^2: 0.5504928637189375\n",
      "Mean Squared Error: 91848500370.03145\n",
      "Parameter set: DecisionTreeRegressor(max_depth=16, min_samples_leaf=30, min_samples_split=44)\n",
      "Test score R^2: 0.7815269691092457\n",
      "Train score R^2: 0.7815269691092457\n",
      "Mean Squared Error: 26856.500884781224\n",
      "Parameter set: DecisionTreeRegressor(max_depth=16, min_samples_leaf=30, min_samples_split=44)\n",
      "Test score R^2: 0.1981026136312567\n",
      "Train score R^2: 0.1981026136312567\n",
      "Mean Squared Error: 9.000973407022101\n",
      "Parameter set: DecisionTreeRegressor(max_depth=16, min_samples_leaf=30, min_samples_split=44)\n",
      "Test score R^2: 0.48146213781166225\n",
      "Train score R^2: 0.48146213781166225\n",
      "Mean Squared Error: 3150.8424354585054\n",
      "Parameter set: DecisionTreeRegressor(max_depth=16, min_samples_leaf=30, min_samples_split=44)\n",
      "Test score R^2: 0.5362524536474875\n",
      "Train score R^2: 0.5362524536474875\n",
      "Mean Squared Error: 94758265764.49924\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.30000000000000004, max_depth=28,\n",
      "                      min_samples_leaf=53, min_samples_split=48)\n",
      "Test score R^2: 0.6441798202604005\n",
      "Train score R^2: 0.6441798202604005\n",
      "Mean Squared Error: 43740.34146474588\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.30000000000000004, max_depth=28,\n",
      "                      min_samples_leaf=53, min_samples_split=48)\n",
      "Test score R^2: 0.14250945833885176\n",
      "Train score R^2: 0.14250945833885176\n",
      "Mean Squared Error: 9.624984060885597\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.30000000000000004, max_depth=28,\n",
      "                      min_samples_leaf=53, min_samples_split=48)\n",
      "Test score R^2: 0.43684482413295955\n",
      "Train score R^2: 0.43684482413295955\n",
      "Mean Squared Error: 3421.9549916404862\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.30000000000000004, max_depth=28,\n",
      "                      min_samples_leaf=53, min_samples_split=48)\n",
      "Test score R^2: 0.4931640505110051\n",
      "Train score R^2: 0.4931640505110051\n",
      "Mean Squared Error: 103562586968.75864\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=23, min_samples_leaf=6,\n",
      "                      min_samples_split=51)\n",
      "Test score R^2: 0.789919150538415\n",
      "Train score R^2: 0.789919150538415\n",
      "Mean Squared Error: 25824.864956727335\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=23, min_samples_leaf=6,\n",
      "                      min_samples_split=51)\n",
      "Test score R^2: 0.11734665711716263\n",
      "Train score R^2: 0.11734665711716263\n",
      "Mean Squared Error: 9.907426314087378\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=23, min_samples_leaf=6,\n",
      "                      min_samples_split=51)\n",
      "Test score R^2: 0.5747918915000031\n",
      "Train score R^2: 0.5747918915000031\n",
      "Mean Squared Error: 2583.733705594328\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=1.0, max_depth=23, min_samples_leaf=6,\n",
      "                      min_samples_split=51)\n",
      "Test score R^2: 0.7423932765727297\n",
      "Train score R^2: 0.7423932765727297\n",
      "Mean Squared Error: 52637187092.92724\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.8, max_depth=9, min_samples_leaf=17,\n",
      "                      min_samples_split=33)\n",
      "Test score R^2: 0.8754858233234442\n",
      "Train score R^2: 0.8754858233234442\n",
      "Mean Squared Error: 15306.306148853106\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.8, max_depth=9, min_samples_leaf=17,\n",
      "                      min_samples_split=33)\n",
      "Test score R^2: 0.11734665711716263\n",
      "Train score R^2: 0.11734665711716263\n",
      "Mean Squared Error: 9.907426314087378\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.8, max_depth=9, min_samples_leaf=17,\n",
      "                      min_samples_split=33)\n",
      "Test score R^2: 0.5826937020887717\n",
      "Train score R^2: 0.5826937020887717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adamd\\_prog\\autoML\\PD\\AutoML_HM1\\solution.ipynb Cell 36\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adamd/_prog/autoML/PD/AutoML_HM1/solution.ipynb#Y156sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m best_test_score \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adamd/_prog/autoML/PD/AutoML_HM1/solution.ipynb#Y156sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, config \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(configuration_grid_decision_tree):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adamd/_prog/autoML/PD/AutoML_HM1/solution.ipynb#Y156sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     results \u001b[39m=\u001b[39m evaluate_pipeline_on_datasets(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adamd/_prog/autoML/PD/AutoML_HM1/solution.ipynb#Y156sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         get_decision_tree_pipeline(), config, train_datasets\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adamd/_prog/autoML/PD/AutoML_HM1/solution.ipynb#Y156sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adamd/_prog/autoML/PD/AutoML_HM1/solution.ipynb#Y156sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     aggregated_test_score \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([result[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adamd/_prog/autoML/PD/AutoML_HM1/solution.ipynb#Y156sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     test_scores\u001b[39m.\u001b[39mappend(aggregated_test_score)\n",
      "File \u001b[1;32mc:\\Users\\adamd\\_prog\\autoML\\PD\\AutoML_HM1\\utills\\pipeline.py:56\u001b[0m, in \u001b[0;36mevaluate_pipeline_on_datasets\u001b[1;34m(pipeline, optimal_config, datasets)\u001b[0m\n\u001b[0;32m     54\u001b[0m results: List[Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m []\n\u001b[0;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m---> 56\u001b[0m     test_score, train_score \u001b[39m=\u001b[39m evaluate_pipeline(\n\u001b[0;32m     57\u001b[0m         pipeline\u001b[39m=\u001b[39;49mpipeline,\n\u001b[0;32m     58\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m     59\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m     60\u001b[0m         X_val\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m     61\u001b[0m         y_val\u001b[39m=\u001b[39;49my,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     results\u001b[39m.\u001b[39mappend((test_score, train_score))\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\adamd\\_prog\\autoML\\PD\\AutoML_HM1\\utills\\pipeline.py:46\u001b[0m, in \u001b[0;36mevaluate_pipeline\u001b[1;34m(pipeline, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest score R^2: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(test_score))\n\u001b[0;32m     45\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain score R^2: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(train_score))\n\u001b[1;32m---> 46\u001b[0m calculate_mse(pipeline, X_val, y_val)\n\u001b[0;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m test_score, train_score\n",
      "File \u001b[1;32mc:\\Users\\adamd\\_prog\\autoML\\PD\\AutoML_HM1\\utills\\pipeline.py:22\u001b[0m, in \u001b[0;36mcalculate_mse\u001b[1;34m(model, X_test, y_test)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my_test must be a pandas Series or numpy array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Generating predictions and calculating MSE\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m     23\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(y_test, predictions)\n\u001b[0;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMean Squared Error: \u001b[39m\u001b[39m{\u001b[39;00mmse\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py:514\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    512\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[0;32m    513\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 514\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[0;32m    515\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\compose\\_column_transformer.py:827\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    823\u001b[0m     \u001b[39m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m    825\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 827\u001b[0m Xs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(\n\u001b[0;32m    828\u001b[0m     X,\n\u001b[0;32m    829\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    830\u001b[0m     _transform_one,\n\u001b[0;32m    831\u001b[0m     fitted\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    832\u001b[0m     column_as_strings\u001b[39m=\u001b[39;49mfit_dataframe_and_transform_dataframe,\n\u001b[0;32m    833\u001b[0m )\n\u001b[0;32m    834\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_output(Xs)\n\u001b[0;32m    836\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m Xs:\n\u001b[0;32m    837\u001b[0m     \u001b[39m# All transformers are None\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\compose\\_column_transformer.py:681\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    675\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    676\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[0;32m    677\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[0;32m    678\u001b[0m     )\n\u001b[0;32m    679\u001b[0m )\n\u001b[0;32m    680\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[0;32m    682\u001b[0m         delayed(func)(\n\u001b[0;32m    683\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[0;32m    684\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    685\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    686\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[0;32m    687\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    688\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    690\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    691\u001b[0m     )\n\u001b[0;32m    692\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_scores = (\n",
    "    []\n",
    ")  # list of test scores calculated across datasets, for each configuration\n",
    "config_ids = []\n",
    "best_config = None\n",
    "best_test_score = float(\"-inf\")\n",
    "\n",
    "for i, config in enumerate(configuration_grid_decision_tree):\n",
    "    results = evaluate_pipeline_on_datasets(\n",
    "        get_decision_tree_pipeline(), config, train_datasets\n",
    "    )\n",
    "\n",
    "    aggregated_test_score = np.mean([result[0] for result in results])\n",
    "    test_scores.append(aggregated_test_score)\n",
    "    config_ids.append(i)\n",
    "\n",
    "    if aggregated_test_score > best_test_score:\n",
    "        best_test_score = aggregated_test_score\n",
    "        best_config = config\n",
    "\n",
    "plt.boxplot(test_scores)\n",
    "plt.xlabel(\"Configuration ID\")\n",
    "plt.ylabel(\"Test Score\")\n",
    "plt.title(\"Test Scores for Decision Tree\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Configuration:\", best_config)\n",
    "print(\"Best Test Score:\", best_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elasticnet_pipeline() -> Pipeline:\n",
    "    elastic_net = ElasticNet(max_iter=10000)\n",
    "    col_trans: ColumnTransformer = get_column_transformer()\n",
    "    decision_tree_pipeline = Pipeline(\n",
    "        steps=[(\"column_transformer\", col_trans), (\"model\", elastic_net)]\n",
    "    )\n",
    "    return decision_tree_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_grid_for_elasticnet():\n",
    "    # parameters space\n",
    "    random.seed(42)\n",
    "    alpha = [i * 0.05 for i in range(21)]\n",
    "    l1_ratio = [i * 0.05 for i in range(21)]\n",
    "    all_combinations = list(\n",
    "        itertools.product(\n",
    "            alpha,\n",
    "            l1_ratio,\n",
    "        )\n",
    "    )\n",
    "    selected_combinations: List[Tuple[float, int, int, int]] = random.sample(\n",
    "        all_combinations, 100\n",
    "    )\n",
    "    parameter_names = [\n",
    "        \"model__alpha\",\n",
    "        \"model__l1_ratio\",\n",
    "    ]\n",
    "\n",
    "    parameters_grid_elasticnet = [\n",
    "        dict(zip(parameter_names, combination)) for combination in selected_combinations\n",
    "    ]\n",
    "    return parameters_grid_elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets: List[Tuple[DataFrame, Series]] = get_train_datasets()\n",
    "elastic_net_pipeline: Pipeline = get_elasticnet_pipeline()\n",
    "parameters_grid_elasticnet = get_parameter_grid_for_elasticnet()\n",
    "optimal_config_elasticnet = find_optimal_configuration_for_all_datasets(\n",
    "    config_space=parameters_grid_elasticnet,\n",
    "    train_datasets=train_datasets,\n",
    "    model=elastic_net_pipeline,\n",
    "    summary_func=np.mean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_config_elasticnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline_on_datasets(\n",
    "    get_elasticnet_pipeline(), optimal_config_elasticnet, train_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_pipeline():\n",
    "    random_forest = RandomForestRegressor()\n",
    "    col_trans: ColumnTransformer = get_column_transformer()\n",
    "    random_forest_pipeline = Pipeline(\n",
    "        steps=[(\"column_transformer\", col_trans), (\"model\", random_forest)]\n",
    "    )\n",
    "    return random_forest_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_grid_for_random_forest():\n",
    "    # parameters space\n",
    "    random.seed(42)\n",
    "    max_depth_values = range(1, 31, 1)\n",
    "    min_samples_split_values = range(2, 61, 1)\n",
    "    min_samples_leaf_values = range(1, 61, 1)\n",
    "    n_estimators_values = range(1, 200, 1)\n",
    "\n",
    "    all_combinations = list(\n",
    "        itertools.product(\n",
    "            max_depth_values,\n",
    "            min_samples_split_values,\n",
    "            min_samples_leaf_values,\n",
    "            n_estimators_values,\n",
    "        )\n",
    "    )\n",
    "    selected_combinations: List[Tuple[float, int, int, int]] = random.sample(\n",
    "        all_combinations, 100\n",
    "    )\n",
    "    parameter_names = [\n",
    "        \"model__max_depth\",\n",
    "        \"model__min_samples_split\",\n",
    "        \"model__min_samples_leaf\",\n",
    "        \"model__n_estimators\",\n",
    "    ]\n",
    "\n",
    "    parameters_grid_random_forest = [\n",
    "        dict(zip(parameter_names, combination)) for combination in selected_combinations\n",
    "    ]\n",
    "    return parameters_grid_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets: List[Tuple[DataFrame, Series]] = get_train_datasets()\n",
    "random_forest_pipeline: Pipeline = get_random_forest_pipeline()\n",
    "parameters_grid_random_forest = get_parameter_grid_for_random_forest()\n",
    "\n",
    "optimal_config_random_forest = find_optimal_configuration_for_all_datasets(\n",
    "    config_space=parameters_grid_random_forest,\n",
    "    train_datasets=train_datasets,\n",
    "    model=random_forest_pipeline,\n",
    "    summary_func=np.mean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_config_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline_on_datasets(\n",
    "    get_random_forest_pipeline(), optimal_config_random_forest, train_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "- dla każdej konfiguracje z paratmers_grid trzeba ją porównać do optymalnej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes - decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('model__ccp_alpha', 0.11), ('model__max_depth', 17), ('model__min_samples_leaf', 2), ('model__min_samples_split', 7)])\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "decision_tree_params = {\n",
    "    \"model__ccp_alpha\": Real(0.11, 1.21, prior=\"log-uniform\"),\n",
    "    \"model__max_depth\": Integer(1, 31, prior=\"log-uniform\"),\n",
    "    \"model__min_samples_split\": Integer(2, 61, prior=\"log-uniform\"),\n",
    "    \"model__min_samples_leaf\": Integer(2, 61, prior=\"log-uniform\"),\n",
    "}\n",
    "\n",
    "out = get_bayes_best_configuration(\n",
    "    get_decision_tree_pipeline(),\n",
    "    [\n",
    "        (decision_tree_params, 30),\n",
    "    ],\n",
    "    X_train_fish_market,\n",
    "    y_train_fish_market,\n",
    "    X_test_fish_market,\n",
    "    y_test_fish_market,\n",
    ")\n",
    "\n",
    "print(out[0])\n",
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9511259336021848\n"
     ]
    }
   ],
   "source": [
    "config = get_bayes_config(\n",
    "    get_decision_tree_pipeline(),\n",
    "    [(decision_tree_params, 40)],\n",
    "    X_train_fish_market,\n",
    "    y_train_fish_market\n",
    ")\n",
    "model = get_decision_tree_pipeline()\n",
    "model.set_params(**config)\n",
    "model.fit(X_train_fish_market, y_train_fish_market)\n",
    "score = model.score(X_test_fish_market, y_test_fish_market)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
