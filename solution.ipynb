{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import openml\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "\n",
    "# from evaluate_pipeline import evaluate_pipeline\n",
    "# from load_dataset import load_dataset_from_id\n",
    "# from perform_optimazaion_of_pipeline_with_random_search import (\n",
    "#     perform_optimazaion_of_pipeline_with_random_search,\n",
    "# )\n",
    "# from split_dataset import split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utill functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimazaion_of_pipeline_with_bayesian_method(\n",
    "    pipeline: Pipeline,\n",
    "    search_space: Dict[str, Any],\n",
    "    X: DataFrame,\n",
    "    y: DataFrame,\n",
    "    n_iter=100,\n",
    ") -> BayesSearchCV:\n",
    "    opt: BayesSearchCV = BayesSearchCV(\n",
    "        pipeline,\n",
    "        # [(space, # of evaluations)]\n",
    "        search_spaces=search_space,\n",
    "        n_iter=n_iter,\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "    )\n",
    "    opt.fit(X, y)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bayes_best_configuration(\n",
    "        pipeline: Pipeline,\n",
    "        search: list[(Dict[str, object], int)],\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        X_test,\n",
    "        Y_test\n",
    "    ):\n",
    "    max = float(\"-inf\")\n",
    "    best = None\n",
    "    iter = None\n",
    "    for config in search:\n",
    "        model = perform_optimazaion_of_pipeline_with_bayesian_method(\n",
    "            pipeline,\n",
    "            [config],\n",
    "            X_train,\n",
    "            Y_train\n",
    "        )\n",
    "        score = model.score(X_test, Y_test)\n",
    "        if max < score:\n",
    "            best = model.best_params_\n",
    "            iter = config[1] \n",
    "    return  (best, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_pipeline_over_params_combinations(\n",
    "    pipeline: Pipeline,\n",
    "    parameters_grid: List[dict],\n",
    "    X: DataFrame,\n",
    "    y: DataFrame,\n",
    "    X_val: DataFrame,\n",
    "    y_val: DataFrame,\n",
    ") -> Pipeline:\n",
    "    # thats the teta^(j)*\n",
    "    best_score = float(\"-inf\")\n",
    "    best_params = None\n",
    "\n",
    "    for params in parameters_grid:\n",
    "        # Update the pipeline parameters\n",
    "        pipeline_params = {f\"{key}\": value for key, value in params.items()}\n",
    "        pipeline.set_params(**pipeline_params)\n",
    "\n",
    "        pipeline.fit(X, y)\n",
    "        score = pipeline.score(X_val, y_val)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = pipeline_params\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(\n",
    "    data: pd.DataFrame, class_: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    X: pd.DataFrame = data.drop(labels=class_, axis=1)\n",
    "    y: pd.DataFrame = data[class_]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_id(id: int) -> DataFrame:\n",
    "    return openml.datasets.get_dataset(dataset_id=id).get_data(\n",
    "        dataset_format=\"dataframe\"\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(model: Pipeline, X_test: pd.DataFrame, y_test: pd.Series):\n",
    "    # Ensure X_test and y_test are the correct types\n",
    "    if not isinstance(X_test, (pd.DataFrame, np.ndarray)):\n",
    "        raise ValueError(\"X_test must be a pandas DataFrame or numpy array\")\n",
    "    if not isinstance(y_test, (pd.Series, np.ndarray)):\n",
    "        raise ValueError(\"y_test must be a pandas Series or numpy array\")\n",
    "\n",
    "    # Generating predictions and calculating MSE\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(\n",
    "    pipeline: Pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "):\n",
    "    if not isinstance(X_val, (pd.DataFrame, np.ndarray)):\n",
    "        raise ValueError(\"X_test must be a pandas DataFrame or numpy array\")\n",
    "    if not isinstance(y_val, (pd.Series, np.ndarray)):\n",
    "        raise ValueError(\"y_test must be a pandas Series or numpy array\")\n",
    "\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    test_score = pipeline.score(X_val, y_val)\n",
    "    train_score = pipeline.score(X, y)\n",
    "    print(\"Parameter set: \" + str(pipeline.named_steps[\"model\"]))\n",
    "    print(\"Test score R^2: \" + str(test_score))\n",
    "    print(\"Train score R^2: \" + str(train_score))\n",
    "    calculate_mse(pipeline, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model: Pipeline, X, y) -> float:\n",
    "    model.fit(X=X, y=y)\n",
    "    return model.score(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    datasets: List[Tuple[DataFrame, Series]], model: Pipeline, config\n",
    ") -> List[float]:\n",
    "    performances: List[float] = []\n",
    "    for X, y in datasets:\n",
    "        model.set_params(**config)\n",
    "        performance: float = evaluate_model_performance(model=model, X=X, y=y)\n",
    "        performances.append(performance)\n",
    "    return performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_configuration_accross_datasets(\n",
    "    config_space,\n",
    "    datasets: List[Tuple[DataFrame, Series]],\n",
    "    model: Pipeline,\n",
    "    summary_func,\n",
    "):\n",
    "    best_config = None\n",
    "    best_summary_score = float(\"0\")\n",
    "\n",
    "    for config in config_space:\n",
    "        performances = experiment(datasets=datasets, model=model, config=config)\n",
    "        summary_score = summary_func(performances)\n",
    "\n",
    "        if summary_score > best_summary_score:\n",
    "            best_summary_score = summary_score\n",
    "            best_config = config\n",
    "\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_transformer() -> ColumnTransformer:\n",
    "    num_pipeline = Pipeline(\n",
    "        steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), (\"scale\", MinMaxScaler())]\n",
    "    )\n",
    "    cat_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"one-hot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    col_trans = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"num_pipeline\",\n",
    "                num_pipeline,\n",
    "                make_column_selector(dtype_include=np.number),\n",
    "            ),\n",
    "            (\"cat_pipeline\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    return col_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline_on_datasets(\n",
    "    pipeline: Pipeline, optimal_config, datasets: List[Tuple[DataFrame, Series]]\n",
    "):\n",
    "    for X, y in datasets:\n",
    "        pipeline.set_params(**optimal_config)\n",
    "        evaluate_pipeline(\n",
    "            pipeline=pipeline,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            X_val=X,\n",
    "            y_val=y,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fish_market_dataset_id = (\n",
    "    43308  # https://www.openml.org/search?type=data&id=43308&sort=runs&status=active\n",
    ")\n",
    "liver_disorders_dataset_id = (\n",
    "    8  # https://www.openml.org/search?type=data&id=8&sort=runs&status=active\n",
    ")\n",
    "diabetes_dataset_id = (\n",
    "    44223  # https://www.openml.org/search?type=data&id=44223&sort=runs&status=active\n",
    ")\n",
    "\n",
    "lisbona_house_prices_dataset_id = (\n",
    "    43660  # https://www.openml.org/search?type=data&id=43660&sort=runs&status=active\n",
    ")\n",
    "\n",
    "\n",
    "fish_market_dataset: DataFrame = load_dataset_from_id(id=fish_market_dataset_id)\n",
    "fish_market_regression_class = \"Weight\"\n",
    "\n",
    "liver_disorders_dataset: DataFrame = load_dataset_from_id(id=liver_disorders_dataset_id)\n",
    "liver_disorders_regression_class = \"drinks\"\n",
    "diabetes_dataset: DataFrame = load_dataset_from_id(id=diabetes_dataset_id)\n",
    "diabetes_regression_class = \"class\"\n",
    "\n",
    "lisbona_house_prices_dataset: DataFrame = load_dataset_from_id(\n",
    "    id=lisbona_house_prices_dataset_id\n",
    ")\n",
    "lisbona_house_prices_regression_class = \"Price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train_fish_market,\n",
    "    X_test_fish_market,\n",
    "    y_train_fish_market,\n",
    "    y_test_fish_market,\n",
    ") = split_dataset(data=fish_market_dataset, class_=fish_market_regression_class)\n",
    "\n",
    "(\n",
    "    X_train_liver_disorders,\n",
    "    X_test_liver_disorders,\n",
    "    y_train_liver_disorders,\n",
    "    y_test_liver_disorders,\n",
    ") = split_dataset(data=liver_disorders_dataset, class_=liver_disorders_regression_class)\n",
    "\n",
    "X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = split_dataset(\n",
    "    diabetes_dataset, diabetes_regression_class\n",
    ")\n",
    "\n",
    "(\n",
    "    X_train_lisbona_house_prices,\n",
    "    X_test_lisbona_house_prices,\n",
    "    y_train_lisbona_house_prices,\n",
    "    y_test_lisbona_house_prices,\n",
    ") = split_dataset(lisbona_house_prices_dataset, lisbona_house_prices_regression_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets() -> List[Tuple[DataFrame, Series]]:\n",
    "    return [\n",
    "        (X_train_fish_market, y_train_fish_market),\n",
    "        (X_train_liver_disorders, y_train_liver_disorders),\n",
    "        (X_train_diabetes, y_train_diabetes),\n",
    "        (X_train_lisbona_house_prices, y_train_lisbona_house_prices),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create generic column transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision_tree_pipeline() -> Pipeline:\n",
    "    decision_tree = DecisionTreeRegressor()\n",
    "    col_trans: ColumnTransformer = get_column_transformer()\n",
    "    decision_tree_pipeline = Pipeline(\n",
    "        steps=[(\"column_transformer\", col_trans), (\"model\", decision_tree)]\n",
    "    )\n",
    "    return decision_tree_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_grid_decision_tree():\n",
    "    # parameters space\n",
    "    ccp_alpha_values = [i * 0.1 for i in range(11)]\n",
    "    max_depth_values = range(1, 31, 1)\n",
    "    min_samples_split_values = range(2, 61, 1)\n",
    "    min_samples_leaf_values = range(1, 61, 1)\n",
    "\n",
    "    all_combinations = list(\n",
    "        itertools.product(\n",
    "            ccp_alpha_values,\n",
    "            max_depth_values,\n",
    "            min_samples_split_values,\n",
    "            min_samples_leaf_values,\n",
    "        )\n",
    "    )\n",
    "    selected_combinations: List[Tuple[float, int, int, int]] = random.sample(\n",
    "        all_combinations, 100\n",
    "    )\n",
    "    parameter_names = [\n",
    "        \"model__ccp_alpha\",\n",
    "        \"model__max_depth\",\n",
    "        \"model__min_samples_split\",\n",
    "        \"model__min_samples_leaf\",\n",
    "    ]\n",
    "\n",
    "    parameters_grid = [\n",
    "        dict(zip(parameter_names, combination)) for combination in selected_combinations\n",
    "    ]\n",
    "    return parameters_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets: List[Tuple[DataFrame, Series]] = get_datasets()\n",
    "decison_tree_pipeline: Pipeline = get_decision_tree_pipeline()\n",
    "parameters_grid_decision_tree = get_parameter_grid_decision_tree()\n",
    "\n",
    "optimal_config_decision_tree = find_optimal_configuration_accross_datasets(\n",
    "    config_space=parameters_grid_decision_tree,\n",
    "    datasets=datasets,\n",
    "    model=decison_tree_pipeline,\n",
    "    summary_func=np.mean,  # Or np.median for a more robust approach\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__ccp_alpha': 0.1, 'model__max_depth': 12, 'model__min_samples_split': 6, 'model__min_samples_leaf': 5}\n"
     ]
    }
   ],
   "source": [
    "print(optimal_config_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=12, min_samples_leaf=5,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.9442644593770035\n",
      "Train score R^2: 0.9442644593770035\n",
      "Mean Squared Error: 6851.4708197725295\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=12, min_samples_leaf=5,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.4677313223280586\n",
      "Train score R^2: 0.4677313223280586\n",
      "Mean Squared Error: 5.9745003469969\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=12, min_samples_leaf=5,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.7948233807576879\n",
      "Train score R^2: 0.7948233807576879\n",
      "Mean Squared Error: 1246.7348014748864\n",
      "Parameter set: DecisionTreeRegressor(ccp_alpha=0.1, max_depth=12, min_samples_leaf=5,\n",
      "                      min_samples_split=6)\n",
      "Test score R^2: 0.8189419523948761\n",
      "Train score R^2: 0.8189419523948761\n",
      "Mean Squared Error: 36995875727.45061\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline_on_datasets(\n",
    "    get_decision_tree_pipeline(), optimal_config_decision_tree, datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elasticnet_pipeline() -> Pipeline:\n",
    "    elastic_net = ElasticNet(max_iter=10000)\n",
    "    col_trans: ColumnTransformer = get_column_transformer()\n",
    "    decision_tree_pipeline = Pipeline(\n",
    "        steps=[(\"column_transformer\", col_trans), (\"model\", elastic_net)]\n",
    "    )\n",
    "    return decision_tree_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_grid_for_elasticnet():\n",
    "    # parameters space\n",
    "    alpha = [i * 0.05 for i in range(21)]\n",
    "    l1_ratio = [i * 0.05 for i in range(21)]\n",
    "    all_combinations = list(\n",
    "        itertools.product(\n",
    "            alpha,\n",
    "            l1_ratio,\n",
    "        )\n",
    "    )\n",
    "    selected_combinations: List[Tuple[float, int, int, int]] = random.sample(\n",
    "        all_combinations, 100\n",
    "    )\n",
    "    parameter_names = [\n",
    "        \"model__alpha\",\n",
    "        \"model__l1_ratio\",\n",
    "    ]\n",
    "\n",
    "    parameters_grid_elasticnet = [\n",
    "        dict(zip(parameter_names, combination)) for combination in selected_combinations\n",
    "    ]\n",
    "    return parameters_grid_elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.864e+11, tolerance: 4.005e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.508e+06, tolerance: 1.561e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.538e+03, tolerance: 3.098e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.025e+06, tolerance: 2.145e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.826e+13, tolerance: 4.005e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.574e+05, tolerance: 1.561e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.203e+03, tolerance: 3.098e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.063e+05, tolerance: 2.145e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+12, tolerance: 4.005e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.574e+05, tolerance: 1.561e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.203e+03, tolerance: 3.098e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.063e+05, tolerance: 2.145e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+12, tolerance: 4.005e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.097e+10, tolerance: 4.005e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.862e+11, tolerance: 4.005e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.092e+10, tolerance: 4.005e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.088e+10, tolerance: 4.005e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.574e+05, tolerance: 1.561e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.203e+03, tolerance: 3.098e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.063e+05, tolerance: 2.145e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+12, tolerance: 4.005e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.864e+11, tolerance: 4.005e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.903e+06, tolerance: 1.561e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.530e+03, tolerance: 3.098e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.981e+05, tolerance: 2.145e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.735e+13, tolerance: 4.005e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.085e+10, tolerance: 4.005e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "datasets: List[Tuple[DataFrame, Series]] = get_datasets()\n",
    "elastic_net_pipeline: Pipeline = get_elasticnet_pipeline()\n",
    "parameters_grid_elasticnet = get_parameter_grid_for_elasticnet()\n",
    "optimal_config_elasticnet = find_optimal_configuration_accross_datasets(\n",
    "    config_space=parameters_grid_elasticnet,\n",
    "    datasets=datasets,\n",
    "    model=elastic_net_pipeline,\n",
    "    summary_func=np.mean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__alpha': 0.0, 'model__l1_ratio': 0.30000000000000004}\n"
     ]
    }
   ],
   "source": [
    "print(optimal_config_elasticnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set: ElasticNet(alpha=0.0, l1_ratio=0.30000000000000004, max_iter=10000)\n",
      "Test score R^2: 0.9285956799012827\n",
      "Train score R^2: 0.9285956799012827\n",
      "Mean Squared Error: 8777.605995988224\n",
      "Parameter set: ElasticNet(alpha=0.0, l1_ratio=0.30000000000000004, max_iter=10000)\n",
      "Test score R^2: 0.22349020246245288\n",
      "Train score R^2: 0.22349020246245288\n",
      "Mean Squared Error: 8.716008003938809\n",
      "Parameter set: ElasticNet(alpha=0.0, l1_ratio=0.30000000000000004, max_iter=10000)\n",
      "Test score R^2: 0.5279193863361497\n",
      "Train score R^2: 0.5279193863361497\n",
      "Mean Squared Error: 2868.549702835578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.574e+05, tolerance: 1.561e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.203e+03, tolerance: 3.098e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.063e+05, tolerance: 2.145e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set: ElasticNet(alpha=0.0, l1_ratio=0.30000000000000004, max_iter=10000)\n",
      "Test score R^2: 0.8448543637107147\n",
      "Train score R^2: 0.8448543637107147\n",
      "Mean Squared Error: 31701151955.04967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+12, tolerance: 4.005e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline_on_datasets(\n",
    "    get_elasticnet_pipeline(), optimal_config_elasticnet, datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_pipeline():\n",
    "    random_forest = RandomForestRegressor()\n",
    "    col_trans: ColumnTransformer = get_column_transformer()\n",
    "    random_forest_pipeline = Pipeline(\n",
    "        steps=[(\"column_transformer\", col_trans), (\"model\", random_forest)]\n",
    "    )\n",
    "    return random_forest_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_grid_for_random_forest():\n",
    "    # parameters space\n",
    "    max_depth_values = range(1, 31, 1)\n",
    "    min_samples_split_values = range(2, 61, 1)\n",
    "    min_samples_leaf_values = range(1, 61, 1)\n",
    "    n_estimators_values = range(1, 200, 1)\n",
    "\n",
    "    all_combinations = list(\n",
    "        itertools.product(\n",
    "            max_depth_values,\n",
    "            min_samples_split_values,\n",
    "            min_samples_leaf_values,\n",
    "            n_estimators_values,\n",
    "        )\n",
    "    )\n",
    "    selected_combinations: List[Tuple[float, int, int, int]] = random.sample(\n",
    "        all_combinations, 100\n",
    "    )\n",
    "    parameter_names = [\n",
    "        \"model__max_depth\",\n",
    "        \"model__min_samples_split\",\n",
    "        \"model__min_samples_leaf\",\n",
    "        \"model__n_estimators\",\n",
    "    ]\n",
    "\n",
    "    parameters_grid_random_forest = [\n",
    "        dict(zip(parameter_names, combination)) for combination in selected_combinations\n",
    "    ]\n",
    "    return parameters_grid_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets: List[Tuple[DataFrame, Series]] = get_datasets()\n",
    "random_forest_pipeline: Pipeline = get_random_forest_pipeline()\n",
    "parameters_grid_random_forest = get_parameter_grid_for_random_forest()\n",
    "\n",
    "optimal_config_random_forest = find_optimal_configuration_accross_datasets(\n",
    "    config_space=parameters_grid_random_forest,\n",
    "    datasets=datasets,\n",
    "    model=random_forest_pipeline,\n",
    "    summary_func=np.mean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_config_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline_on_datasets(\n",
    "    get_random_forest_pipeline(), optimal_config_random_forest, datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes - decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_params = {\n",
    "    \"model__ccp_alpha\": Real(0.11, 1.21, prior=\"log-uniform\"),\n",
    "    \"model__max_depth\": Integer(1, 31, prior=\"log-uniform\"),\n",
    "    \"model__min_samples_split\": Integer(2, 61, prior=\"log-uniform\"),\n",
    "    \"model__min_samples_leaf\": Integer(2, 61, prior=\"log-uniform\"),\n",
    "}\n",
    "\n",
    "decision_tree_params = {\n",
    "    \"model__ccp_alpha\": Real(0.11, 1.21, prior=\"log-uniform\"),\n",
    "    \"model__max_depth\": Integer(1,31, prior=\"log-uniform\"),\n",
    "    \"model__min_samples_split\": Integer(2,61, prior=\"log-uniform\"),\n",
    "    \"model__min_samples_leaf\": Integer(2,61, prior=\"log-uniform\"),\n",
    "}\n",
    "\n",
    "out = get_bayes_best_configuration(\n",
    "    get_decision_tree_pipeline(),\n",
    "    [(decision_tree_params, 30), (decision_tree_params, 30), (decision_tree_params, 30), (decision_tree_params, 30)],\n",
    "    X_train_fish_market,\n",
    "    y_train_fish_market,\n",
    "    X_test_fish_market,\n",
    "    y_test_fish_market\n",
    ")\n",
    "\n",
    "print(out[0])\n",
    "print(out[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
